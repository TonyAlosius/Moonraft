# -*- coding: utf-8 -*-
"""Moonraft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jV9wtPvnlAZllEKk4ELR_sZpn3AaJncM

Here, I have implemented a product similarity search functionality using TF-IDF vectorization and cosine similarity. The process involved reading data from CSV files, preprocessing the text data by removing stopwords and punctuation, calculating the cosine similarity between a query and a list of products, and finally, returning a list of ***similar products based on a specified threshold***. The code utilized libraries such as NLTK, scikit-learn, and CSV for data processing, vectorization, and similarity calculation.

1. amazon_data = ***fetch_data***('/content/drive/MyDrive/ColabNotebooks/amazon.csv', 3)
2. amazon_data = ***preprocess_data***(amazon_data)
3. Repeat ***Step 1*** and ***Step 2*** for all the datasets
4. similar_products_amazon = ***search_similar_products***(query, amazon_data, threshold)
5. similar_products_currys = ***search_similar_products***(query, currys_data, threshold)

## 1. The code mounts Google Drive to access the required CSV files
"""

from google.colab import drive
drive.mount('/content/drive')

"""## 2. Import the Necessary Libraries

1. TfidfVectorizer - the TF-IDF vectorizer is used to convert the textual data from the Lewis, Amazon and Currys datasets into numerical feature vectors. Specifically, the TF-IDF vectorizer is applied to the product names in the datasets.

2. cosine_similarity - Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space. It calculates the cosine of the angle between the two vectors and determines how similar they are based on their orientations.
"""

import csv
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""## 3. Fetch the Product Name out of the data

1. The fetch_data function is responsible for extracting data from a CSV file. It takes the file path (file_route) and the Product name of the column (feature) from which the data needs to be extracted.

2. It opens the CSV file and creates a CSV reader object to iterate over the rows. For each row, it accesses the value in the specified column (row[feature]) and appends it to the data list.

3. Finally, the function returns the data list containing the extracted data from the CSV file.
"""

def fetch_data(file_route, feature):
    data = []
    with open(file_route, 'r', encoding='utf-8') as file_name:
        data_file = csv.reader(file_name)
        for row in data_file:
            # Appends the product names onto the data list
            data.append(row[feature])
    return data

"""## 4. Preprocess the Product Names using NLTK Toolkit

The preprocess_data function takes a list of product names from the scraped data as input and performs text preprocessing tasks, including tokenization, removal of stopwords and punctuation, and joining tokens back into a preprocessed string. It returns a list of preprocessed text for each input element.
"""

import nltk
import string

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

def preprocess_data(data):
    nltk.download('stopwords')
    nltk.download('punkt')
    stop_words = set(stopwords.words('english'))
    punctuation = set(string.punctuation)
    preprocessed_data = []
    for row in data:
        tokens = word_tokenize(row)
        tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token not in punctuation]
        preprocessed_row = ' '.join(tokens)
        preprocessed_data.append(preprocessed_row)
    return preprocessed_data

"""Workbox of the above function"""

# ### NLTK ToolKit Workbox ###
# nltk.download('stopwords')
# nltk.download('punkt')
# stop_words = set(stopwords.words('english'))
# punctuation = set(string.punctuation)
# # print(stop_words)
# # print(punctuation)
# for row in currys_data:
#   preprocessed_data = []
#   tokens = word_tokenize(row)
#   # print(tokens)
#   tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token not in punctuation]
#   preprocessed_row = ' '.join(tokens)
#   # print(preprocessed_row)
#   preprocessed_data.append(preprocessed_row)
#   # print(preprocessed_data)
# print(tfidf_matrix[:-1].shape)
# print(tfidf_matrix[-1].shape)

"""## 5. Calculate the Similarity Score

The calculate_similarity function uses a TfidfVectorizer to convert text data of product name searched for and the list of product lists into TF-IDF vectors. It then calculates the cosine similarity between the query and each product, returning the similarity scores.
"""

def calculate_similarity(searched_prod_name, products):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(products + [searched_prod_name])
    similarity_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])
    return similarity_scores[0]

"""Workbox of the above function"""

# ### Cosine Similarity Workbox ###
# query = john_lewis_data[21]
# products = currys_data
# vectorizer = TfidfVectorizer()
# tfidf_matrix = vectorizer.fit_transform(products + [query])
# print(tfidf_matrix)
# print(tfidf_matrix.shape)
# print(products + [query])
# feature_names = vectorizer.get_feature_names_out()
# for row, col in zip(*tfidf_matrix.nonzero()):
#     word = feature_names[col]
#     tfidf_value = tfidf_matrix[row, col]
#     # print(f"Row: {row}, Col: {col}, Word: {word}, TF-IDF Value: {tfidf_value}")
# similarity_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])
# print(similarity_scores)
# print(similarity_scores.shape)
# print(similarity_scores[0])

"""## 6. Search the Similar Products based on Similarity score

The search_similar_products function takes a query, a list of products, and a threshold as input. It calculates the similarity scores between the query and each product using the calculate_similarity function. Then, it filters the products based on the threshold and returns a list of similar products.
"""

def search_similar_products(searched_prod_name, products, threshold):
    similarity_scores = calculate_similarity(searched_prod_name, products)
    similar_products = []
    for i, score in enumerate(similarity_scores):
        if score >= threshold:
            similar_products.append(products[i])
    return similar_products

"""## 7. Fetch and Preprocess the data"""

amazon_data = fetch_data('/content/drive/MyDrive/Colab Notebooks/amazon.csv', 3)
amazon_data = amazon_data[1:]
amazon_data = preprocess_data(amazon_data)

currys_data = fetch_data('/content/drive/MyDrive/Colab Notebooks/currys.csv', 2)
currys_data = currys_data[1:]
currys_data = preprocess_data(currys_data)

john_lewis_data = fetch_data('/content/drive/MyDrive/Colab Notebooks/johnlewis.csv', 2)
john_lewis_data = john_lewis_data[1:]
john_lewis_data = preprocess_data(john_lewis_data)

"""############## Select the Product name to search #########################"""

searched_prod_name = john_lewis_data[21]

"""## 8. Search the product name and fetch the similar products from other two sites"""

searched_prod_name = john_lewis_data[21]
threshold = 0.3

similar_products_amazon = search_similar_products(searched_prod_name, amazon_data, threshold)
similar_products_currys = search_similar_products(searched_prod_name, currys_data, threshold)

print("Similar products in Amazon:")
for product in similar_products_amazon:
    print(product)

print("\nSimilar products in Currys:")
for product in similar_products_currys:
    print(product)